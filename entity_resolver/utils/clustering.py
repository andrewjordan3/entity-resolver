# entity_resolver/utils/clustering.py
"""
This module provides advanced, GPU-accelerated clustering operations, including
post-processing steps to refine cluster assignments. These functions are designed
to work with vector embeddings and label arrays generated by clustering algorithms
like HDBSCAN.
"""

import cupy
import logging
from typing import Dict, List, Tuple
from cuml.neighbors import NearestNeighbors
from cuml.metrics.pairwise_distances import pairwise_distances

# Set up a logger for this module.
logger = logging.getLogger(__name__)


def _evaluate_noise_point_attachment(
    point_neighbor_labels: cupy.ndarray,
    point_similarities: cupy.ndarray,
    tau: float,
    min_matching: int,
    ratio_threshold: float
) -> int:
    """
    Evaluates if a single noise point should be attached to an existing cluster.

    This helper function contains the core logic for the ratio and strength tests,
    determining if a noise point's connection to a potential cluster is strong
    and unambiguous enough to warrant attachment.

    Args:
        point_neighbor_labels: The cluster labels of the neighbors.
        point_similarities: The similarity scores of the neighbors.
        tau: The minimum similarity threshold for a neighbor to be considered a match.
        min_matching: The minimum number of matching neighbors required.
        ratio_threshold: The minimum ratio of similarity between the best and
                         second-best cluster candidates.

    Returns:
        The integer label of the cluster to attach to, or -1 if no attachment
        should be made.
    """
    # Filter out neighbors that are also noise points.
    valid_neighbor_mask = point_neighbor_labels != -1
    if not valid_neighbor_mask.any():
        return -1  # No non-noise neighbors, cannot attach.

    valid_labels = point_neighbor_labels[valid_neighbor_mask]
    valid_sims = point_similarities[valid_neighbor_mask]

    # Find the most frequent non-noise neighbor clusters.
    unique_candidate_labels, counts = cupy.unique(valid_labels, return_counts=True)
    sorted_indices = cupy.argsort(counts)[::-1]
    sorted_candidate_labels = unique_candidate_labels[sorted_indices]

    best_candidate_label = int(sorted_candidate_labels[0])

    # --- Ratio Test: Ensure the best choice is significantly better than the second best ---
    if len(sorted_candidate_labels) > 1:
        second_best_candidate_label = sorted_candidate_labels[1]

        sims_to_best = valid_sims[valid_labels == best_candidate_label]
        mean_sim_to_best = sims_to_best.mean()

        sims_to_second_best = valid_sims[valid_labels == second_best_candidate_label]
        mean_sim_to_second_best = sims_to_second_best.mean()

        # If the similarity ratio is too low, the choice is ambiguous.
        # The small epsilon (1e-8) prevents division by zero.
        if mean_sim_to_best / (mean_sim_to_second_best + 1e-8) < ratio_threshold:
            return -1  # Ambiguous attachment, leave as noise.

    # --- Strength Test: Ensure the connection to the best candidate is strong enough ---
    similarities_to_best_candidate = valid_sims[valid_labels == best_candidate_label]
    num_strong_matches = (similarities_to_best_candidate >= tau).sum()

    # The point must have enough neighbors in the candidate cluster, the average
    # similarity must meet the threshold, and the number of strong matches
    # must also meet the minimum requirement.
    if (similarities_to_best_candidate.size >= min_matching and
            similarities_to_best_candidate.mean() >= tau and
            num_strong_matches >= min_matching):
        return best_candidate_label

    return -1  # Failed strength test, leave as noise.


def attach_noise_points(
    vectors: cupy.ndarray,
    labels: cupy.ndarray,
    k: int,
    tau: float,
    min_matching: int,
    ratio_threshold: float
) -> cupy.ndarray:
    """
    Attaches noise points (-1) to existing clusters if they have a strong connection.

    This GPU-accelerated procedure identifies noise points, finds their nearest
    neighbors, and applies a series of tests to determine if they can be
    unambiguously assigned to a nearby cluster.

    Args:
        vectors: The embedding vectors for all data points.
        labels: The initial cluster labels array, containing -1 for noise points.
        k: The number of nearest neighbors to consider.
        tau: The similarity threshold for a strong connection.
        min_matching: The minimum number of neighbors that must belong to a
                      candidate cluster.
        ratio_threshold: A threshold for the ratio test to ensure the best
                         candidate cluster is significantly better than the second best.

    Returns:
        An updated labels array with noise points re-assigned where appropriate.
    """
    noise_indices = cupy.where(labels == -1)[0]
    if len(noise_indices) == 0:
        logger.info("No noise points found to attach.")
        return labels

    logger.info(f"Attempting to attach {len(noise_indices)} noise points.")

    # Find k+1 neighbors because the point itself will be included in the results.
    nn_model = NearestNeighbors(n_neighbors=k + 1, metric='cosine').fit(vectors)
    distances, neighbor_indices = nn_model.kneighbors(vectors[noise_indices])

    # Drop the first neighbor (the point itself) and convert distance to similarity.
    similarities = 1 - distances[:, 1:]
    neighbor_indices = neighbor_indices[:, 1:]
    neighbor_labels = labels[neighbor_indices]

    final_labels = labels.copy()
    attachments = 0

    # This loop iterates over the noise points to evaluate them one by one.
    # The heavy computation (k-NN) is already done on the GPU.
    for i, original_noise_index in enumerate(noise_indices.tolist()):
        new_label = _evaluate_noise_point_attachment(
            point_neighbor_labels=neighbor_labels[i],
            point_similarities=similarities[i],
            tau=tau,
            min_matching=min_matching,
            ratio_threshold=ratio_threshold
        )
        if new_label != -1:
            final_labels[original_noise_index] = new_label
            attachments += 1

    logger.info(f"Successfully attached {attachments} out of {len(noise_indices)} noise points.")
    return final_labels


# --- Cluster Merging Helper Functions ---

def _calculate_cluster_centroids(
    vectors: cupy.ndarray,
    labels: cupy.ndarray,
    cluster_ids: List[int],
    sample_size: int
) -> Tuple[cupy.ndarray, Dict[int, cupy.ndarray]]:
    
    """Calculates centroids for each cluster, sampling if clusters are large."""

    logger.debug(f"Calculating centroids for {len(cluster_ids)} clusters.")
    cluster_centroids = {}
    cluster_members = {}
    for cid in cluster_ids:
        members = cupy.where(labels == cid)[0]
        cluster_members[cid] = members
        if len(members) > sample_size:
            sample_indices = cupy.random.choice(members, sample_size, replace=False)
            cluster_centroids[cid] = cupy.mean(vectors[sample_indices], axis=0)
        else:
            cluster_centroids[cid] = cupy.mean(vectors[members], axis=0)
    
    centroid_matrix = cupy.vstack([cluster_centroids[cid] for cid in cluster_ids])

    return centroid_matrix, cluster_members


def _find_candidate_pairs(
    centroid_matrix: cupy.ndarray,
    cluster_ids: List[int],
    similarity_threshold: float,
    batch_size: int
) -> List[Tuple[int, int]]:
    
    """Efficiently finds candidate pairs for merging based on centroid similarity."""

    n_clusters = len(cluster_ids)
    logger.debug(f"Finding merge candidates from {n_clusters} clusters using batch size {batch_size}.")
    merge_candidates = []
    for start_idx in range(0, n_clusters, batch_size):
        end_idx = min(start_idx + batch_size, n_clusters)
        batch_centroids = centroid_matrix[start_idx:end_idx]
        
        similarities = 1 - pairwise_distances(batch_centroids, centroid_matrix, metric='cosine')
        
        # Find pairs with similarity above the threshold.
        high_sim_pairs = cupy.where(similarities > similarity_threshold)
        
        for i, j in zip(high_sim_pairs[0].tolist(), high_sim_pairs[1].tolist()):
            # Map batch index back to the original cluster index.
            actual_i = start_idx + i
            # Ensure we only check each pair once (j > i) to avoid duplicates.
            if j > actual_i:
                merge_candidates.append((cluster_ids[actual_i], cluster_ids[j]))
    
    potential_pairs = n_clusters * (n_clusters - 1) // 2
    logger.info(f"Pre-filtering reduced {potential_pairs} potential pairs to {len(merge_candidates)} candidates.")
    
    return merge_candidates


def _perform_detailed_check_and_union(
    candidate_pairs: List[Tuple[int, int]],
    vectors: cupy.ndarray,
    cluster_members: Dict[int, cupy.ndarray],
    union_find: Dict[int, int],
    params: Dict
):
    """Performs detailed similarity checks on candidate pairs and merges them."""
    
    # --- Union-Find Helper Functions ---
    def find_set_root(cluster_id):
        path = []
        while union_find[cluster_id] != cluster_id:
            path.append(cluster_id)
            cluster_id = union_find[cluster_id]
        # Path compression for efficiency
        for node in path:
            union_find[node] = cluster_id
        return cluster_id

    def union_sets(id1, id2):
        root1 = find_set_root(id1)
        root2 = find_set_root(id2)
        if root1 != root2:
            union_find[root1] = root2

    # --- Detailed Check Logic ---
    merges_made = 0
    for id1, id2 in candidate_pairs:
        members1 = cluster_members[id1]
        members2 = cluster_members[id2]
        
        # Sample points from each cluster for the detailed comparison.
        sample1 = vectors[cupy.random.choice(members1, min(len(members1), params['merge_sample_size']), replace=False)]
        sample2 = vectors[cupy.random.choice(members2, min(len(members2), params['merge_sample_size']), replace=False)]

        similarity_matrix = 1 - pairwise_distances(sample1, sample2, metric='cosine')
        
        if similarity_matrix.size > 0:
            median_similarity = float(cupy.median(similarity_matrix))
            max_similarity = float(cupy.max(similarity_matrix))
            
            if median_similarity >= params['merge_median_threshold'] and max_similarity >= params['merge_max_threshold']:
                union_sets(id1, id2)
                merges_made += 1
    logger.info(f"Performed {len(candidate_pairs)} detailed checks, resulting in {merges_made} merges.")


def _relabel_from_union_find(labels: cupy.ndarray, union_find: Dict[int, int]) -> cupy.ndarray:
    """Applies the final cluster mappings from the Union-Find structure."""
    
    # --- Union-Find Helper ---
    def find_set_root(cluster_id):
        path = []
        while union_find[cluster_id] != cluster_id:
            path.append(cluster_id)
            cluster_id = union_find[cluster_id]
        for node in path:
            union_find[node] = cluster_id
        return cluster_id

    final_mapping = {cid: find_set_root(cid) for cid in union_find.keys()}
    final_labels = labels.copy()
    
    for original_label, new_label in final_mapping.items():
        if original_label != new_label:
            final_labels[labels == original_label] = new_label
    return final_labels


def merge_snn_clusters(
    vectors: cupy.ndarray,
    labels: cupy.ndarray,
    merge_params: Dict
) -> cupy.ndarray:
    """
    Merges clusters based on inter-cluster similarity using a multi-stage approach.

    This function first uses fast centroid comparisons to find potential merge
    candidates, then performs a more detailed, sample-based similarity check on
    those candidates to make a final merge decision.

    Args:
        vectors: The embedding vectors for all data points.
        labels: The initial cluster labels array.
        merge_params: A dictionary of parameters controlling the merge process.

    Returns:
        An updated labels array with similar clusters merged.
    """
    unique_labels = cupy.unique(labels)
    cluster_ids = [c for c in unique_labels.tolist() if c != -1]
    if len(cluster_ids) < 2:
        logger.info("Fewer than two clusters exist; no merging is possible.")
        return labels

    logger.info(f"Starting merge process for {len(cluster_ids)} clusters.")

    # 1. Calculate centroids for an efficient pre-filtering step.
    centroid_matrix, cluster_members = _calculate_cluster_centroids(
        vectors, labels, cluster_ids, merge_params['centroid_sample_size']
    )

    # 2. Find promising candidate pairs using fast centroid similarity.
    candidate_pairs = _find_candidate_pairs(
        centroid_matrix, cluster_ids, merge_params['centroid_similarity_threshold'], merge_params['merge_batch_size']
    )

    if not candidate_pairs:
        logger.info("No promising candidate pairs found after pre-filtering. No merges will be made.")
        return labels

    # 3. Perform detailed checks on candidates and merge using a Union-Find structure.
    union_find = {cid: cid for cid in cluster_ids}
    _perform_detailed_check_and_union(
        candidate_pairs, vectors, cluster_members, union_find, merge_params
    )

    # 4. Apply the merges by relabeling the points based on the Union-Find results.
    final_labels = _relabel_from_union_find(labels, union_find)

    final_cluster_count = len(cupy.unique(final_labels[final_labels != -1]))
    logger.info(f"Cluster merging complete. Final cluster count: {final_cluster_count}.")
    
    return final_labels
